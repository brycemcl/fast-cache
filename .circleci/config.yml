version: 2.1
commands:
  save_cache_zstd:
    parameters:
      paths:
        description: 'List of directories which should be added to the cache'
        type: string
        default: '"."'
      key:
        description: 'Unique identifier for this cache'
        type: string
      displayName:
        description: 'Title of the step to be shown in the CircleCI UI'
        type: string
        default: 'Saving Cache'
      when:
        description: 'Specify when to enable or disable the step. Takes the following values: always, on_success, on_fail (default: on_success)'
        type: string
        default: 'on_success'
    steps:
      - run:
          name: << parameters.displayName >>
          when: << parameters.when >>
          command: |
            #!/bin/bash
            echo "Uploading << parameters.paths >> to << parameters.key >>"
            set -e
            start=$(date +%s)
            mkdir -p << parameters.paths >>
            tar -cP << parameters.paths >> |
              zstd -2 --long --threads=4 - |
              rclone rcat \
                --buffer-size=16Mi \
                --checkers=16 \
                --fast-list \
                --ignore-checksum \
                --streaming-upload-cutoff=100Ki \
                --transfers=40 \
                --use-mmap \
                "cache:fast-cache/<< parameters.key >>.tar.zstd"
            end=$(date +%s)
            runtime=$((end - start))
            echo "Upload: $runtime seconds"
            echo "cache:fast-cache/<< parameters.key >>.tar.zstd"
  restore_cache_zstd:
    parameters:
      keys:
        description: 'List of cache keys to lookup for a cache to restore. Only first existing key will be restored.'
        type: string
      displayName:
        description: 'Title of the step to be shown in the CircleCI UI'
        type: string
        default: 'Restoring Cache'
    steps:
      - run:
          name: << parameters.displayName >>
          environment:
            KEYS: << parameters.keys >>
          command: |
            #!/bin/bash

            find_objects_in_bucket() {
              # Loop through all arguments passed to the function
              for param in "$@"; do
                # Try to find an exact match by adding the .tar.zstd prefix
                exact_match="$(rclone lsjson --fast-list --files-only -R cache:fast-cache/${param}.tar.zstd | jq -e '.[].Path')"

                # If an exact match is found
                if [ -n "$exact_match" ]; then
                  echo "Exact match found: $(dirname $param)/$exact_match"
                  process_objects "$(dirname $param)/$exact_match"
                  return
                else
                  # Else, find a prefix match sorted by ModTime
                  prefix_match="$(rclone lsjson --fast-list --files-only -R cache:fast-cache/ --include "${param}*" | jq -e 'sort_by(.ModTime) | last | .Path')"

                  if [ -n "$prefix_match" ]; then
                    echo "Prefix match found: $(dirname $param)/$prefix_match"
                    process_objects "$(dirname $param)/$prefix_match"
                    return
                  else
                    echo "No match found for $param"
                  fi
                fi
              done
            }

            process_objects() {
              local object=$1
              rclone cat \
                --buffer-size=16Mi \
                --checkers=16 \
                --fast-list \
                --ignore-checksum \
                --transfers=40 \
                --use-mmap \
                "cache:fast-cache/$object" |
                zstd -d - |
                tar -xP --skip-old-files
            }

            # Call the function with all script arguments
            echo "Looking for cache keys: $KEYS"
            find_objects_in_bucket $KEYS

jobs:
  zstd:
    parameters:
      working_directory:
        description: 'Working directory for the job'
        type: string
    resource_class: 'arm.large'
    working_directory: << parameters.working_directory >>
    docker:
      - image: node:18-slim
    steps:
      - run: npm install pnpm -g
      - run: apt-get update && apt-get install -y git openssh-client
      - checkout
      - run: pnpm install
      # - run: pnpm install --ignore-scripts bloater
      # - run: pnpm run start:createData
      - run: apt-get install -y tar zstd rclone jq
      - run: rclone config create cache "google cloud storage" location=us-east1 bucket_policy_only=true env_auth=true
      - save_cache_zstd:
          key: 'v1-<< parameters.working_directory >>-<< pipeline.git.revision >>'
          paths: 'node_modules/'
      - run: rm -fr node_modules
      - restore_cache_zstd:
          keys: 'v1-<< parameters.working_directory >>-<< pipeline.git.revision >>'
      - run: rm -fr node_modules
      - restore_cache_zstd:
          keys: 'v1-<< parameters.working_directory >>'
  circleci:
    parameters:
      working_directory:
        description: 'Working directory for the job'
        type: string
    resource_class: 'arm.large'
    working_directory:  << parameters.working_directory >>
    docker:
      - image: node:18-slim
    steps:
      - run: npm install pnpm -g
      - run: apt-get update && apt-get install -y git openssh-client
      - checkout
      - run: pnpm install
      # - run: pnpm install --ignore-scripts bloater
      # - run: pnpm run start:createData
      - save_cache:
          key: v1-<< parameters.working_directory >>-{{ .Environment.CIRCLE_SHA1 }}
          paths:
            - node_modules/
      - run: rm -fr node_modules
      - restore_cache:
          keys:
            - v1-<< parameters.working_directory >>-{{ .Environment.CIRCLE_SHA1 }}

workflows:
  comparison:
    jobs:
      - zstd:
          matrix:
            parameters:
              working_directory:
                - /mnt/ramdisk
                # - /root/project
      # - circleci:
      #     matrix:
      #       parameters:
      #         working_directory:
      #           - /mnt/ramdisk
      #           - /root/project